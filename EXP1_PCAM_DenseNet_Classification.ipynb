{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.set_logical_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.LogicalDeviceConfiguration(memory_limit=1024)])\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_pth = \".\\Datasets\\camelyonpatch_level_2_split_train_x.h5\"\n",
    "train_labels_pth = \".\\Datasets\\camelyonpatch_level_2_split_train_y.h5\"\n",
    "test_images_pth = \".\\Datasets\\camelyonpatch_level_2_split_valid_x.h5\"\n",
    "test_labels_pth = \".\\Datasets\\camelyonpatch_level_2_split_valid_y.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(train_images_pth,swmr=True) as train_x:\n",
    "    h5train_images = train_x['x']\n",
    "    train_images = np.asarray(h5train_images)\n",
    "\n",
    "with h5py.File(train_labels_pth) as train_y:\n",
    "    h5train_labels = train_y['y']\n",
    "    train_labels = np.asarray(h5train_labels)\n",
    "\n",
    "with h5py.File(test_images_pth) as valid_x:\n",
    "    h5validation_images = valid_x['x']\n",
    "    validation_images = np.asarray(h5validation_images)\n",
    "\n",
    "with h5py.File(test_labels_pth) as valid_y:\n",
    "    h5validation_labels = valid_y['y']\n",
    "    validation_labels = np.asarray(h5validation_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANKITH MOTHA - 21BAI1284\n",
      "\n",
      "Dataset Info:\n",
      "Training Images: (262144, 96, 96, 3)\n",
      "Training Labels: (262144, 1, 1, 1)\n",
      "Validation Images: (32768, 96, 96, 3)\n",
      "Validation Labels: (32768, 1, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"ANKITH MOTHA - 21BAI1284\\n\")\n",
    "print(\"Dataset Info:\")\n",
    "print(\"Training Images:\",train_images.shape)\n",
    "print(\"Training Labels:\",train_labels.shape)\n",
    "print(\"Validation Images:\",validation_images.shape)\n",
    "print(\"Validation Labels:\",validation_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(131072, 32768)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_train_images = np.asarray([i for i in train_images[:len(train_images)//2]],dtype=\"float16\")\n",
    "reshaped_validation_images = np.asarray([i for i in validation_images],dtype=\"float16\")\n",
    "del train_images, validation_images\n",
    "\n",
    "len_train_images = reshaped_train_images.shape[0]\n",
    "len_validation_images = reshaped_validation_images.shape[0]\n",
    "\n",
    "len_train_images,len_validation_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((131072,), (32768,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squeezed_train_labels = np.squeeze(train_labels[:len(train_labels)//2])\n",
    "squeezed_validation_labels = np.squeeze(validation_labels)\n",
    "\n",
    "# squeezed_train_labels = train_labels[:len(train_labels)//2]\n",
    "# squeezed_validation_labels = validation_labels\n",
    "squeezed_train_labels.shape,squeezed_validation_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "# with tf.device(\"/device:GPU:0\"):\n",
    "#     reshaped_train_images = tf.convert_to_tensor(reshaped_train_images,dtype=tf.float16)\n",
    "#     reshaped_validation_images = tf.convert_to_tensor(reshaped_validation_images,dtype=tf.float16)\n",
    "#     squeezed_train_labels = tf.convert_to_tensor(squeezed_train_labels)\n",
    "#     squeezed_train_labels = tf.convert_to_tensor(squeezed_validation_labels)\n",
    "\n",
    "# train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "# train_datagen.flow(reshaped_train_images, squeezed_train_labels, batch_size=batch_size)\n",
    "# del reshaped_train_images\n",
    "\n",
    "# validation_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "# validation_datagen.flow(reshaped_validation_images, squeezed_validation_labels, batch_size=batch_size)\n",
    "# del reshaped_validation_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 17\u001b[0m\n\u001b[0;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     13\u001b[0m         loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     14\u001b[0m         metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     16\u001b[0m EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m\n\u001b[1;32m---> 17\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreshaped_train_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43msqueezed_train_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlen_train_images\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreshaped_validation_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43msqueezed_validation_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlen_validation_images\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Software\\Python\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32md:\\Software\\Python\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "with tf.device(\"/device:GPU:0\"):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Flatten(input_shape=(96,96,3)))\n",
    "    model.add(tf.keras.layers.Dense(256, activation='relu',))\n",
    "    model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "    EPOCHS = 15\n",
    "    history = model.fit(\n",
    "        reshaped_train_images,squeezed_train_labels,\n",
    "        steps_per_epoch=len_train_images // batch_size,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=(reshaped_validation_images,squeezed_validation_labels),\n",
    "        validation_steps=len_validation_images// batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CAT and DOG Classification - 21BAI1284"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \".\\Datasets\\Cats_Dogs_Kaggle\"\n",
    "train_dir = os.path.join(base_dir, 'training_set')\n",
    "validation_dir = os.path.join(base_dir, 'test_set')\n",
    "\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')  # directory with our training cat pictures\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')  # directory with our training dog pictures\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')  # directory with our validation cat pictures\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')  # directory with our validation dog pictures\n",
    "\n",
    "#Understanding the data\n",
    "num_cats_tr = len(os.listdir(train_cats_dir))\n",
    "num_dogs_tr = len(os.listdir(train_dogs_dir))\n",
    "\n",
    "num_cats_val = len(os.listdir(validation_cats_dir))\n",
    "num_dogs_val = len(os.listdir(validation_dogs_dir))\n",
    "\n",
    "total_train = num_cats_tr + num_dogs_tr\n",
    "total_val = num_cats_val + num_dogs_val\n",
    "\n",
    "print('total training cat images:', num_cats_tr)\n",
    "print('total training dog images:', num_dogs_tr)\n",
    "\n",
    "print('total validation cat images:', num_cats_val)\n",
    "print('total validation dog images:', num_dogs_val)\n",
    "print(\"--\")\n",
    "print(\"Total training images:\", total_train)\n",
    "print(\"Total validation images:\", total_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE  = 150\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_image_generator      = ImageDataGenerator(rescale=1./255)\n",
    "validation_image_generator = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_data_gen = train_image_generator.flow_from_directory(directory=train_dir,\n",
    "                                                        batch_size = BATCH_SIZE,\n",
    "                                                        shuffle=True,\n",
    "                                                        target_size=(IMG_SHAPE,IMG_SHAPE),\n",
    "                                                        class_mode='binary')\n",
    "\n",
    "val_data_gen = validation_image_generator.flow_from_directory(directory=validation_dir,\n",
    "                                                        batch_size = BATCH_SIZE,\n",
    "                                                        shuffle=False,\n",
    "                                                        target_size=(IMG_SHAPE,IMG_SHAPE),\n",
    "                                                        class_mode='binary')\n",
    "\n",
    "sample_training_images, sample_training_labels = next(train_data_gen)\n",
    "label_names = [\"CAT\",\"DOG\"]\n",
    "\n",
    "def plotImages(images_arr,labels):\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(20,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, labs, ax in zip(images_arr,labels, axes):\n",
    "        ax.imshow(img)\n",
    "        ax.text(0.5,-0.2, str(label_names[int(labs)])+\"\\n21BAI1284\", size=12, ha=\"center\",transform=ax.transAxes)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "plotImages(sample_training_images[:5],sample_training_labels[:5])\n",
    "\n",
    "print(sample_training_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ANKITH MOTHA - 21BAI1284\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape=(IMG_SHAPE,IMG_SHAPE, 3)))\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "EPOCHS = 25\n",
    "history = model.fit(\n",
    "    train_data_gen,\n",
    "    steps_per_epoch=total_train // BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_data_gen,\n",
    "    validation_steps=total_val// BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(EPOCHS)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('ANKITH - Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('ANKITH - Training and Validation Loss')\n",
    "plt.savefig('./foo.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
